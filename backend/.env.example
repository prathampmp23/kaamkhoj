# Ollama LLM Configuration

# Base URL for the Ollama API service
OLLAMA_API_URL=http://localhost:11434

# Default model to use for entity extraction
# Recommended: llama3, mistral, llama2
DEFAULT_MODEL=llama3

# LLM generation parameters
# Lower temperature means more deterministic outputs (better for extraction)
TEMPERATURE=0.1
MAX_TOKENS=100

# Toggle to enable/disable LLM-based extraction
ENABLE_LLM_EXTRACTION=true

# Fallback to rule-based extraction when LLM fails or is unavailable
ENABLE_RULE_BASED_FALLBACK=true

# Language detection parameters
ENABLE_AUTO_LANGUAGE_DETECTION=true
DEFAULT_LANGUAGE=en-IN