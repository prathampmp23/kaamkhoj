# Voice-Based AI Assistant Setup Guide

## Overview
Your KaamKhoj application now features a **fully local voice-based AI assistant** that works completely offline for illiterate users. The system uses:

- **Ollama Qwen2.5:7b** - Local LLM for intelligent entity extraction
- **Whisper.cpp** - Offline speech-to-text (STT)
- **Coqui TTS** - Offline text-to-speech (TTS)
- **Multi-language support** - Hindi & English

---

## Architecture Flow

```
User Voice Input
     ‚Üì
Frontend (MediaRecorder) ‚Üí Records 5 seconds of audio
     ‚Üì
POST /stt (Backend) ‚Üí FFmpeg converts WebM to WAV
     ‚Üì
Whisper.cpp ‚Üí Transcribes audio to text
     ‚Üì
POST /process (Backend) ‚Üí Ollama extracts field data
     ‚Üì
Response to Frontend ‚Üí Updates form + speaks confirmation
     ‚Üì
POST /tts (Backend) ‚Üí Coqui TTS generates speech
     ‚Üì
Frontend plays audio response
```

---

## Backend Setup (Completed ‚úÖ)

### 1. Dependencies Installed
```json
{
  "express": "^5.1.0",
  "multer": "^2.0.2",
  "mongoose": "^8.18.0",
  "cors": "^2.8.5"
}
```

### 2. Endpoints Implemented

#### `/stt` - Speech to Text
- **Method:** POST
- **Input:** Audio file (WebM format from browser)
- **Process:**
  1. Receives audio via `multer`
  2. Converts WebM ‚Üí WAV using `ffmpeg`
  3. Runs Whisper.cpp for transcription
  4. Detects language
- **Output:** `{ text: string, lang: string }`

#### `/process` - Entity Extraction
- **Method:** POST
- **Input:** `{ text, lang, currentField, retryCount }`
- **Process:**
  1. Sends text to Ollama Qwen2.5:7b
  2. Extracts specific field (name, age, phone, etc.)
  3. Validates extraction
- **Output:** `{ reply, extractedValue, success }`

#### `/tts` - Text to Speech
- **Method:** POST
- **Input:** `{ text, lang }`
- **Process:**
  1. Calls Python `synthesize.py` script
  2. Generates WAV file using Coqui TTS
  3. Streams audio back to client
- **Output:** Audio WAV stream

---

## Frontend Changes (Completed ‚úÖ)

### 1. Replaced Browser Speech Recognition
**Before:** Used browser's `webkitSpeechRecognition` (requires internet)
**After:** Uses `MediaRecorder` API ‚Üí uploads to server

### 2. Server-Side TTS
**Before:** Used browser's `speechSynthesis` (limited voice quality)
**After:** Uses Coqui TTS via `/tts` endpoint (high-quality multilingual)

### 3. Enhanced UI/UX for Illiterate Users

#### Large, Visual Buttons
- **Speak Button:** 70px height, large microphone icon üé§
- **Repeat Button:** Green color, speaker icon üîä
- **Animated feedback** when recording (pulsing red effect)

#### Button States
- **Listening:** Red pulsing animation
- **Speaking:** Button disabled with visual feedback
- **Ready:** Blue gradient with hover effects

---

## System Requirements

### Backend Server
1. **Node.js** (v16+)
2. **MongoDB** (running on localhost:27017)
3. **Ollama** with Qwen2.5:7b model
4. **FFmpeg** (for audio conversion)
5. **Whisper.cpp** (for STT)
6. **Python 3.11** with virtual environment
7. **Coqui TTS** library

### Installation Checklist

#### ‚úÖ 1. FFmpeg
```bash
# Download from https://www.gyan.dev/ffmpeg/builds/
# Extract to C:\ffmpeg
# Add C:\ffmpeg\bin to PATH
ffmpeg -version  # Test
```

#### ‚úÖ 2. Whisper.cpp
```bash
# Download from https://github.com/ggerganov/whisper.cpp/releases
# Extract to C:\whisper
# Download model: ggml-base.en.bin
# Place in C:\whisper\models\
```

#### ‚úÖ 3. Python Environment
```bash
cd backend
py -3.11 -m venv .venv
source .venv/Scripts/activate  # Git Bash
pip install TTS torch torchvision torchaudio
```

#### ‚úÖ 4. Ollama
```bash
# Install from https://ollama.com
ollama pull qwen2.5:7b
ollama serve  # Runs on http://localhost:11434
```

---

## Running the Application

### 1. Start Backend Services

#### Terminal 1: MongoDB
```bash
mongod
```

#### Terminal 2: Ollama
```bash
ollama serve
```

#### Terminal 3: Backend Server
```bash
cd backend
node app.js
# Server running on http://localhost:5000
```

### 2. Start Frontend
```bash
cd frontend
npm run dev
# Frontend running on http://localhost:5173
```

---

## Usage Flow for Illiterate Users

### Step-by-Step Process

1. **User opens AI Assistant page**
   - Sees large "Tap to Speak" button üé§
   - Hears first question automatically

2. **User taps microphone button**
   - Button turns red and pulses
   - Records for 5 seconds
   - Shows "Listening..." feedback

3. **System processes audio**
   - Uploads to `/stt`
   - Transcribes with Whisper
   - Shows transcribed text in chat

4. **Ollama extracts information**
   - Identifies field value (e.g., name, age)
   - Validates extraction
   - Updates form automatically

5. **System speaks confirmation**
   - "Thank you, I've recorded your name as..."
   - Asks next question
   - User can repeat if needed

6. **Form completion**
   - All fields filled via voice
   - Large "Submit All Information" button
   - Confirmation message spoken

---

## Multi-Language Support

### Automatic Language Detection
- Detects Hindi vs English from speech
- Uses appropriate TTS voice
- Maintains language throughout session

### Hindi Support
- Questions asked in Hindi
- TTS speaks in Hindi voice
- Form labels in Devanagari script
- Example: "‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?"

### English Support
- Questions in English
- English TTS voice
- Standard form labels
- Example: "What is your name?"

---

## Ollama Integration

### Model Configuration
```javascript
const ollamaService = new OllamaService('http://localhost:11434');
```

### Entity Extraction Prompts
The system sends structured prompts to Qwen2.5:7b:

```
SYSTEM: You are an extraction assistant.
TASK: Extract the field: <FIELD_NAME>
Return JSON with keys: field, value, success.

USER: "User said: <USER_TEXT>"

OUTPUT: {"field":"name","value":"Rahul Sharma","success":true}
```

### Supported Fields
- `name` - Full name extraction
- `gender` - Male/Female/Other
- `age` - Numeric age
- `phone` - 10-digit Indian phone numbers
- `address` - Full address with city/state
- `workExperience` - Years of experience
- `skills` - Job skills
- `availability` - Work schedule preferences

---

## Advanced Features

### 1. Retry Logic
- Tracks failed extractions per field
- Provides progressively clearer instructions
- Example:
  - Retry 1: "I didn't catch your name. Please say it clearly."
  - Retry 2: "Let's try again. Say 'My name is...'"
  - Retry 3: "Just state your name without other words."

### 2. Manual Input Fallback
- Users can type if voice fails
- Auto-advances to next field on blur
- Debounced to prevent premature submission

### 3. Visual Feedback
- Active field highlighted in blue
- Pulsing animation while recording
- Disabled state when system is speaking

### 4. Conversation History
- Displays full chat transcript
- Color-coded messages (user vs assistant)
- Auto-scrolls to latest message

---

## Troubleshooting

### Problem: TTS not working
**Solution:**
1. Check Python environment is activated
2. Verify TTS installed: `pip list | grep TTS`
3. Test synthesize.py manually:
   ```bash
   python synthesize.py --text "Hello" --out test.wav --lang en
   ```

### Problem: Whisper not transcribing
**Solution:**
1. Verify Whisper path in `app.js`:
   ```javascript
   const whisperBin = 'C:\\whisper\\main.exe';
   ```
2. Test whisper manually:
   ```bash
   C:\whisper\main.exe -m C:\whisper\models\ggml-base.en.bin -f test.wav
   ```

### Problem: Ollama not extracting
**Solution:**
1. Check Ollama is running: `curl http://localhost:11434`
2. Verify model pulled: `ollama list`
3. Check OllamaService logs in backend console

### Problem: FFmpeg conversion fails
**Solution:**
1. Verify FFmpeg in PATH: `ffmpeg -version`
2. Check uploads folder exists and is writable
3. Review backend console for FFmpeg errors

---

## Performance Optimization

### 1. Model Selection
- **Whisper:** `ggml-base.en.bin` for English-only (faster)
- **Whisper:** `ggml-medium.bin` for multilingual (more accurate)
- **TTS:** XTTS v2 model (high quality, ~2GB)

### 2. Response Time
- STT processing: 2-5 seconds (depends on audio length)
- Ollama extraction: 1-3 seconds (Qwen2.5:7b)
- TTS generation: 1-2 seconds per sentence

### 3. Caching
- Ollama keeps model in memory after first use
- TTS model loads once per server start
- Whisper model stays in memory

---

## Security & Privacy

### ‚úÖ Fully Offline
- No data sent to external APIs
- All processing happens locally
- User data never leaves the machine

### ‚úÖ Data Protection
- Audio files deleted after processing
- TTS files deleted after streaming
- MongoDB stores final form data only

### ‚úÖ No API Keys Required
- No Google Cloud Speech API
- No OpenAI API
- No third-party dependencies

---

## Future Enhancements

### 1. Voice Confirmation
Add confirmation step before submitting each field:
```javascript
"Is Rahul Kumar correct? Say YES or NO"
```

### 2. Known Person Cards
Pre-filled profiles for returning users:
- Display photo cards
- Tap to auto-fill
- Voice confirms identity

### 3. Job Matching
Real-time job recommendations based on extracted profile:
- Match skills with job requirements
- Filter by location and availability
- Speak matching jobs aloud

### 4. Multi-Step Forms
Break complex fields into simpler questions:
- Address: Ask house number, then street, then city
- Experience: Ask duration, then previous employer

---

## Support & Documentation

### Backend Files
- `backend/app.js` - Main Express server with all endpoints
- `backend/synthesize.py` - Coqui TTS Python script
- `backend/services/OllamaService.js` - Ollama integration
- `backend/services/LanguageDetectionService.js` - Language detection

### Frontend Files
- `frontend/src/pages/AiAssistantPage.jsx` - Main voice assistant UI
- `frontend/src/pages/AiAssistantPage.css` - Enhanced styling
- `frontend/src/components/AiAssistant.jsx` - Preview component

### Configuration
- MongoDB: `mongodb://localhost:27017/kaamkhoj`
- Backend: `http://localhost:5000`
- Ollama: `http://localhost:11434`
- Frontend: `http://localhost:5173`

---

## Contact & Support

For issues or questions:
1. Check backend console logs
2. Check frontend browser console
3. Verify all services are running
4. Test endpoints individually using Postman

---

**Last Updated:** November 29, 2025
**Version:** 1.0.0
**Status:** Production Ready ‚úÖ
